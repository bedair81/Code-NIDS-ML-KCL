{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, time\n",
    "import cuml\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading:  Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Friday-WorkingHours-Morning.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Wednesday-workingHours.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Monday-WorkingHours.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Tuesday-WorkingHours.pcap_ISCX_Relabeled.csv\n",
      "Reading:  Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_Relabeled.csv\n",
      "Reading:  CTU13_Normal_Traffic_Relabeled.csv\n",
      "Reading:  CTU13_Attack_Traffic_Relabeled.csv\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV files, and merging all of them into a single DataFrame\n",
    "CISIDS2017_folder = \"/home/grassfed37/6CCS3PRJ/dummy-ML_NIDS/CICIDS2017ML\"\n",
    "CTU13_folder = \"/home/grassfed37/6CCS3PRJ/dummy-ML_NIDS/CTU13ML\"\n",
    "\n",
    "CICIDS2017_df_list = []\n",
    "CTU13_df_list = []\n",
    "\n",
    "# Reading CICIDS2017 CSV files into a single DataFrame\n",
    "for f in os.listdir(CISIDS2017_folder):\n",
    "    file_path = os.path.join(CISIDS2017_folder, f)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(\"Reading: \", f)\n",
    "        CICIDS2017_df_list.append(pd.read_csv(file_path))\n",
    "\n",
    "# Reading CTU13 CSV files into a single DataFrame        \n",
    "for f in os.listdir(CTU13_folder):\n",
    "    file_path = os.path.join(CTU13_folder, f)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(\"Reading: \", f)\n",
    "        CTU13_df_list.append(pd.read_csv(file_path))\n",
    "        \n",
    "CICIDS2017_df, CTU13_df = pd.concat(CICIDS2017_df_list, ignore_index=True), pd.concat(CTU13_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with problematic values:  [' Flow Packets/s']\n",
      "Columns with problematic values:  []\n"
     ]
    }
   ],
   "source": [
    "# QUICK PREPROCESSING. \n",
    "# Some classifiers do not like \"infinite\" (inf) or \"null\" (NaN) values.\n",
    "CICIDS2017_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(\"Columns with problematic values: \", list(CICIDS2017_df.columns[CICIDS2017_df.isna().any()]))\n",
    "CICIDS2017_df.dropna(inplace=True)\n",
    "\n",
    "CTU13_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(\"Columns with problematic values: \", list(CTU13_df.columns[CTU13_df.isna().any()]))\n",
    "CTU13_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'PortScan', 'Bot', 'DoS slowloris', 'DoS Slowhttptest',\n",
       "       'DoS Hulk', 'DoS GoldenEye', 'Heartbleed',\n",
       "       'Web Attack � Brute Force', 'Web Attack � XSS',\n",
       "       'Web Attack � Sql Injection', 'DDoS', 'FTP-Patator', 'SSH-Patator',\n",
       "       'Infiltration'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CICIDS2017_df[' Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'Bot'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTU13_df[' Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that unifies all malicious classes into a single class for binary classification\n",
    "CICIDS2017_df['GT'] = np.where(CICIDS2017_df[' Label']=='BENIGN', 'Benign', 'Malicious')\n",
    "CTU13_df['GT'] = np.where(CTU13_df[' Label']=='BENIGN', 'Benign', 'Malicious')\n",
    "\n",
    "# Fit and transform 'GT' column for CICIDS2017_df\n",
    "CICIDS2017_df['GT'] = LabelEncoder().fit_transform(CICIDS2017_df['GT'])\n",
    "\n",
    "# Fit and transform 'GT' column for CTU13_df \n",
    "CTU13_df['GT'] = LabelEncoder().fit_transform(CTU13_df['GT'])\n",
    "\n",
    "# Perform label encoding on the 'Label' column for each DataFrame separately\n",
    "CICIDS2017_df[' Label'] = LabelEncoder().fit_transform(CICIDS2017_df[' Label'])\n",
    "CTU13_df[' Label'] = LabelEncoder().fit_transform(CTU13_df[' Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.Index([\n",
    "    ' Flow Duration',\n",
    "    ' Total Fwd Packets',\n",
    "    ' Total Backward Packets',\n",
    "    ' Total Length of Bwd Packets',\n",
    "    ' Fwd Packet Length Max',\n",
    "    ' Fwd Packet Length Min',\n",
    "    ' Fwd Packet Length Mean',\n",
    "    ' Fwd Packet Length Std',\n",
    "    ' Bwd Packet Length Min',\n",
    "    ' Bwd Packet Length Mean',\n",
    "    ' Bwd Packet Length Std',\n",
    "    ' Flow Packets/s',\n",
    "    ' Flow IAT Mean',\n",
    "    ' Flow IAT Std',\n",
    "    ' Flow IAT Max',\n",
    "    ' Flow IAT Min',\n",
    "    ' Fwd IAT Mean',\n",
    "    ' Fwd IAT Std',\n",
    "    ' Fwd IAT Max',\n",
    "    ' Fwd IAT Min',\n",
    "    ' Bwd IAT Mean',\n",
    "    ' Bwd IAT Std',\n",
    "    ' Bwd IAT Max',\n",
    "    ' Bwd IAT Min',\n",
    "    ' Bwd PSH Flags',\n",
    "    ' Fwd Header Length',\n",
    "    ' Bwd Header Length',\n",
    "    ' Bwd Packets/s',\n",
    "    ' Min Packet Length',\n",
    "    ' Max Packet Length',\n",
    "    ' Packet Length Mean',\n",
    "    ' Packet Length Std',\n",
    "    ' Packet Length Variance',\n",
    "    ' SYN Flag Count',\n",
    "    ' RST Flag Count',\n",
    "    ' ACK Flag Count',\n",
    "    ' Down/Up Ratio',\n",
    "    ' Average Packet Size',\n",
    "    ' Avg Fwd Segment Size',\n",
    "    ' Avg Bwd Segment Size',\n",
    "    ' Init_Win_bytes_backward',\n",
    "    ' act_data_pkt_fwd',\n",
    "    ' Active Std',\n",
    "    ' Active Max',\n",
    "    ' Active Min',\n",
    "    ' Idle Std',\n",
    "    ' Idle Max',\n",
    "    ' Idle Min'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode feature columns to float32\n",
    "CTU13_df[features] = CTU13_df[features].astype('float32')\n",
    "CTU13_df['GT'] = CTU13_df['GT'].astype('float32')\n",
    "CICIDS2017_df[features] = CICIDS2017_df[features].astype('float32')\n",
    "CICIDS2017_df['GT'] = CICIDS2017_df['GT'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (CTU13 Binary):  1.1096422672271729\n",
      "Model saved to:  ../Pickle Files/SVMBinaryCTU13.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "start = time.time()\n",
    "svmClf_bin_ctu13 = cuml.svm.SVC()\n",
    "svmClf_bin_ctu13.fit(train_ctu13[features].values, train_ctu13['GT'].values)\n",
    "end = time.time() - start\n",
    "print(\"Training time (CTU13 Binary): \", end)\n",
    "\n",
    "# Save the binary SVM model trained on CTU13 dataset\n",
    "with open('../Pickle Files/SVMBinaryCTU13.pkl', 'wb') as file:\n",
    "    pickle.dump(svmClf_bin_ctu13, file)\n",
    "    print(\"Model saved to: \", file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (CICIDS2017 Binary):  185.79507637023926\n",
      "Model saved to:  ../Pickle Files/SVMBinaryCICIDS2017.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "start = time.time()  \n",
    "svmClf_bin_cicids = cuml.svm.SVC()\n",
    "svmClf_bin_cicids.fit(train_cicids[features].values, train_cicids['GT'].values)\n",
    "end = time.time() - start\n",
    "print(\"Training time (CICIDS2017 Binary): \", end)\n",
    "\n",
    "# Save the binary SVM model trained on CICIDS2017 dataset  \n",
    "with open('../Pickle Files/SVMBinaryCICIDS2017.pkl', 'wb') as file:\n",
    "    pickle.dump(svmClf_bin_cicids, file)\n",
    "    print(\"Model saved to: \", file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [22:49:39.527976] Warning: could not fill working set, found only 521 elements\n",
      "[W] [22:49:39.532532] Warning: could not fill working set, found only 770 elements\n",
      "[W] [22:49:39.535893] Warning: could not fill working set, found only 773 elements\n",
      "[W] [22:49:39.539013] Warning: could not fill working set, found only 771 elements\n",
      "[W] [22:49:39.542012] Warning: could not fill working set, found only 772 elements\n",
      "[W] [22:49:39.591161] Warning: could not fill working set, found only 521 elements\n",
      "[W] [22:49:39.595240] Warning: could not fill working set, found only 771 elements\n",
      "[W] [22:49:39.597900] Warning: could not fill working set, found only 774 elements\n",
      "[W] [22:49:39.600072] Warning: could not fill working set, found only 773 elements\n",
      "[W] [22:49:39.602883] Warning: could not fill working set, found only 776 elements\n",
      "[W] [22:49:39.836520] Warning: could not fill working set, found only 535 elements\n",
      "[W] [22:49:39.840389] Warning: could not fill working set, found only 770 elements\n",
      "[W] [22:49:39.844561] Warning: could not fill working set, found only 777 elements\n",
      "[W] [22:49:39.848333] Warning: could not fill working set, found only 800 elements\n",
      "[W] [22:49:39.851896] Warning: could not fill working set, found only 773 elements\n",
      "[W] [22:49:39.854962] Warning: could not fill working set, found only 778 elements\n",
      "[W] [22:49:39.901133] Warning: could not fill working set, found only 535 elements\n",
      "[W] [22:49:39.903416] Warning: could not fill working set, found only 772 elements\n",
      "[W] [22:49:39.904906] Warning: could not fill working set, found only 779 elements\n",
      "[W] [22:49:39.906289] Warning: could not fill working set, found only 772 elements\n",
      "[W] [22:49:39.908500] Warning: could not fill working set, found only 784 elements\n",
      "[W] [22:49:39.909783] Warning: could not fill working set, found only 775 elements\n",
      "Training time (CICIDS2017 Multiclass):  159.04099297523499\n",
      "Model saved to:  ../Pickle Files/SVMMulticlassCICIDS2017.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Train a Multiclass SVM model on CICIDS2017 dataset    \n",
    "start = time.time()\n",
    "svmClf_multi_cicids = cuml.svm.SVC()\n",
    "svmClf_multi_cicids.fit(train_cicids[features].values, train_cicids[' Label'].values)\n",
    "end = time.time() - start\n",
    "print(\"Training time (CICIDS2017 Multiclass): \", end)\n",
    "\n",
    "# Save the multiclass SVM model trained on CICIDS2017 dataset\n",
    "with open('../Pickle Files/SVMMulticlassCICIDS2017.pkl', 'wb') as file:  \n",
    "    pickle.dump(svmClf_multi_cicids, file)\n",
    "    print(\"Model saved to: \", file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the binary SVM model trained on CTU13 dataset\n",
    "with open('../Pickle Files/SVMBinaryCTU13.pkl', 'rb') as file:\n",
    "    svmClf_bin_ctu13 = pickle.load(file)\n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Create a SHAP explainer object for the binary SVM model trained on CTU13 dataset\n",
    "\n",
    "explainer_ctu13 = cuml.explainer.KernelExplainer(model=svmClf_bin_ctu13.predict, data=train_ctu13[features].values)\n",
    "\n",
    "print(\"SHAP explainer created\")\n",
    "\n",
    "# Get SHAP values against the CTU13 test set\n",
    "shap_values_ctu13 = explainer_ctu13.shap_values(test_ctu13[features].values)\n",
    "print(\"SHAP values calculated against CTU13 test set\")\n",
    "\n",
    "# Save the SHAP values to a .pkl file\n",
    "with open('../Pickle Files/shap_values_SVMBinaryCTU13_CTU13.pkl', 'wb') as file:\n",
    "    pickle.dump(shap_values_ctu13, file)\n",
    "    print(\"SHAP values saved to: \", file.name)\n",
    "\n",
    "# Get SHAP values against the CICIDS2017 test set\n",
    "shap_values_cicids = explainer_ctu13.shap_values(test_cicids[features].values)\n",
    "print(\"SHAP values calculated against CICIDS2017 test set\")\n",
    "\n",
    "# Save the SHAP values to a .pkl file\n",
    "with open('../Pickle Files/shap_values_SVMBinaryCTU13_CICIDS2017.pkl', 'wb') as file:\n",
    "    pickle.dump(shap_values_cicids, file)\n",
    "    print(\"SHAP values saved to: \", file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the binary SVM model trained on CICIDS2017 dataset\n",
    "with open('../Pickle Files/SVMBinaryCICIDS2017.pkl', 'rb') as file:\n",
    "    svmClf_bin_cicids = pickle.load(file)\n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Create a SHAP explainer object for the binary SVM model trained on CICIDS2017 dataset\n",
    "\n",
    "explainer_cicids = cuml.explainer.KernelExplainer(model=svmClf_bin_cicids.predict, data=train_cicids[features].values)\n",
    "print(\"SHAP explainer created\")\n",
    "\n",
    "# Get SHAP values against the CTU13 test set\n",
    "shap_values_ctu13 = explainer_cicids.shap_values(test_ctu13[features].values)\n",
    "print(\"SHAP values calculated against CTU13 test set\")\n",
    "\n",
    "# Save the SHAP values to a .pkl file\n",
    "with open('../Pickle Files/shap_values_SVMBinaryCICIDS2017_CTU13.pkl', 'wb') as file:\n",
    "    pickle.dump(shap_values_ctu13, file)\n",
    "    print(\"SHAP values saved to: \", file.name)\n",
    "\n",
    "# Get SHAP values against the CICIDS2017 test set\n",
    "shap_values_cicids = explainer_cicids.shap_values(test_cicids[features].values)\n",
    "print(\"SHAP values calculated against CICIDS2017 test set\")\n",
    "\n",
    "# Save the SHAP values to a .pkl file\n",
    "with open('../Pickle Files/shap_values_SVMBinaryCICIDS2017_CICIDS2017.pkl', 'wb') as file:\n",
    "    pickle.dump(shap_values_cicids, file)\n",
    "    print(\"SHAP values saved to: \", file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the binary SVM model trained on CTU13 dataset\n",
    "with open('../Pickle Files/SVMBinaryCTU13.pkl', 'rb') as file:\n",
    "    svmClf_bin_ctu13 = pickle.load(file)\n",
    "    print(\"Model loaded from: \", file.name)\n",
    "    \n",
    "# Test on CTU13 dataset    \n",
    "predictions_bin_ctu13 = svmClf_bin_ctu13.predict(test_ctu13[features])\n",
    "\n",
    "# Now compute the metrics\n",
    "print(\"Acc (CTU13 Binary) SVM: {:.3f}\".format(accuracy_score(test_ctu13['GT'], predictions_bin_ctu13)))  \n",
    "print(\"Precision (CTU13 Binary) SVM: {:.3f}\".format(precision_score(test_ctu13['GT'], predictions_bin_ctu13, pos_label=1)))\n",
    "print(\"Recall (CTU13 Binary) SVM: {:.3f}\".format(recall_score(test_ctu13['GT'], predictions_bin_ctu13, pos_label=1)))\n",
    "print(\"F1-score (CTU13 Binary) SVM: {:.3f}\".format(f1_score(test_ctu13['GT'], predictions_bin_ctu13, pos_label=1)))\n",
    "\n",
    "# Confusion Matrix\n",
    "pd.crosstab(test_ctu13['GT'], predictions_bin_ctu13, rownames=['True'], colnames=['Pred']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT']) \n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the binary SVM model trained on CICIDS2017 dataset\n",
    "with open('../Pickle Files/SVMBinaryCICIDS2017.pkl', 'rb') as file:  \n",
    "    svmClf_bin_cicids = pickle.load(file)\n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Test on CICIDS2017 dataset\n",
    "predictions_bin_cicids = svmClf_bin_cicids.predict(test_cicids[features])  \n",
    "print(\"Acc (CICIDS2017 Binary) SVM: {:3f}\".format(accuracy_score(test_cicids['GT'], predictions_bin_cicids)))\n",
    "print(\"Precision (CICIDS2017 Binary) SVM: {:3f}\".format(precision_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))  \n",
    "print(\"Recall (CICIDS2017 Binary) SVM: {:3f}\".format(recall_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))\n",
    "print(\"F1-score (CICIDS2017 Binary) SVM: {:3f}\".format(f1_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))\n",
    "pd.crosstab(test_cicids['GT'], predictions_bin_cicids, rownames=['True'], colnames=['Pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT']) \n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the multiclass SVM model trained on CICIDS2017 dataset\n",
    "with open('../Pickle Files/SVMMulticlassCICIDS2017.pkl', 'rb') as file:\n",
    "    svmClf_multi_cicids = pickle.load(file)  \n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Test on CICIDS2017 dataset\n",
    "predictions_multi_cicids = svmClf_multi_cicids.predict(test_cicids[features])\n",
    "print(\"Acc (CICIDS2017 Multiclass) SVM: {:3f}\".format(accuracy_score(test_cicids[' Label'], predictions_multi_cicids)))\n",
    "print(\"Precision (CICIDS2017 Multiclass) SVM: {:3f}\".format(precision_score(test_cicids[' Label'], predictions_multi_cicids, average='macro')))  \n",
    "print(\"Recall (CICIDS2017 Multiclass) SVM: {:3f}\".format(recall_score(test_cicids[' Label'], predictions_multi_cicids, average='macro'))) \n",
    "print(\"F1-score (CICIDS2017 Multiclass) SVM: {:3f}\".format(f1_score(test_cicids[' Label'], predictions_multi_cicids, average='macro')))\n",
    "pd.crosstab(test_cicids[' Label'], predictions_multi_cicids, rownames=['True'], colnames=['Pred'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets\n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT']) \n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT'])\n",
    "\n",
    "# Load the binary SVM model trained on CTU13 dataset\n",
    "with open('../Pickle Files/SVMBinaryCTU13.pkl', 'rb') as file:\n",
    "    svmClf_bin_ctu13 = pickle.load(file)  \n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Test on CICIDS2017 dataset\n",
    "predictions_bin_cicids = svmClf_bin_ctu13.predict(test_cicids[features])\n",
    "print(\"Acc (CTU13 to CICIDS2017) SVM: {:3f}\".format(accuracy_score(test_cicids['GT'], predictions_bin_cicids))) \n",
    "print(\"Precision (CTU13 to CICIDS2017) SVM: {:3f}\".format(precision_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))\n",
    "print(\"Recall (CTU13 to CICIDS2017) SVM: {:3f}\".format(recall_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))\n",
    "print(\"F1-score (CTU13 to CICIDS2017) SVM: {:3f}\".format(f1_score(test_cicids['GT'], predictions_bin_cicids, pos_label=1)))\n",
    "pd.crosstab(test_cicids['GT'], predictions_bin_cicids, rownames=['True'], colnames=['Pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splits for both datasets \n",
    "train_ctu13, test_ctu13 = train_test_split(CTU13_df, test_size=0.4, stratify=CTU13_df['GT'])\n",
    "train_cicids, test_cicids = train_test_split(CICIDS2017_df, test_size=0.4, stratify=CICIDS2017_df['GT']) \n",
    "\n",
    "# Load the binary SVM model trained on CICIDS2017\n",
    "with open('../Pickle Files/SVMBinaryCICIDS2017.pkl', 'rb') as file:  \n",
    "    svmClf_bin = pickle.load(file)\n",
    "    print(\"Model loaded from: \", file.name)\n",
    "\n",
    "# Test on CTU13 dataset  \n",
    "predictions_bin = svmClf_bin.predict(test_ctu13[features])\n",
    "print(\"Acc (CICIDS2017 Binary to CTU13) SVM: {:3f}\".format(accuracy_score(test_ctu13['GT'], predictions_bin)))\n",
    "print(\"Precision (CICIDS2017 Binary to CTU13) SVM: {:3f}\".format(precision_score(test_ctu13['GT'], predictions_bin, pos_label=1)))\n",
    "print(\"Recall (CICIDS2017 Binary to CTU13) SVM: {:3f}\".format(recall_score(test_ctu13['GT'], predictions_bin, pos_label=1)))\n",
    "print(\"F1-score (CICIDS2017 Binary to CTU13) SVM: {:3f}\".format(f1_score(test_ctu13['GT'], predictions_bin, pos_label=1)))\n",
    "pd.crosstab(test_ctu13['GT'], predictions_bin, rownames=['True'], colnames=['Pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
